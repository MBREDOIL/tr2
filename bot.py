import logging
from pyrogram import Client, filters
from pyrogram.handlers import MessageHandler
import requests
import hashlib
import json
import os
from apscheduler.schedulers.background import BackgroundScheduler
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from datetime import datetime

# ‡§≤‡•â‡§ó‡§ø‡§Ç‡§ó ‡§∏‡•á‡§ü‡§Ö‡§™
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

USER_DATA_FILE = 'user_data.json'

def get_domain(url):
    """URL ‡§∏‡•á ‡§°‡•ã‡§Æ‡•á‡§® ‡§®‡§ø‡§ï‡§æ‡§≤‡•á‡§Ç"""
    parsed_uri = urlparse(url)
    return f"{parsed_uri.netloc}"

def load_user_data():
    """‡§Ø‡•Ç‡§ú‡§º‡§∞ ‡§°‡•á‡§ü‡§æ ‡§≤‡•ã‡§° ‡§ï‡§∞‡•á‡§Ç"""
    try:
        with open(USER_DATA_FILE, 'r') as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return {}

def save_user_data(user_data):
    """‡§Ø‡•Ç‡§ú‡§º‡§∞ ‡§°‡•á‡§ü‡§æ ‡§∏‡•á‡§µ ‡§ï‡§∞‡•á‡§Ç"""
    with open(USER_DATA_FILE, 'w') as f:
        json.dump(user_data, f, indent=4)

def fetch_url_content(url):
    """‡§µ‡•á‡§¨‡§∏‡§æ‡§á‡§ü ‡§ï‡§Ç‡§ü‡•á‡§Ç‡§ü ‡§´‡§º‡•á‡§ö ‡§ï‡§∞‡•á‡§Ç"""
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        return response.text
    except Exception as e:
        logger.error(f"Error fetching {url}: {e}")
        return None

def extract_documents(html_content, base_url):
    """HTML ‡§∏‡•á ‡§°‡•â‡§ï‡•ç‡§Ø‡•Ç‡§Æ‡•á‡§Ç‡§ü ‡§≤‡§ø‡§Ç‡§ï ‡§®‡§ø‡§ï‡§æ‡§≤‡•á‡§Ç"""
    soup = BeautifulSoup(html_content, 'lxml')
    document_extensions = ['.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx', '.txt']
    documents = []
    
    for link in soup.find_all('a', href=True):
        href = link['href']
        absolute_url = urljoin(base_url, href)
        link_text = link.text.strip()
        
        if any(absolute_url.lower().endswith(ext) for ext in document_extensions):
            # ‡§≤‡§ø‡§Ç‡§ï ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§Ø‡§æ ‡§´‡§º‡§æ‡§á‡§≤‡§®‡§æ‡§Æ ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç
            if not link_text:
                filename = os.path.basename(absolute_url)
                link_text = os.path.splitext(filename)[0]
            
            documents.append({
                'name': link_text,
                'url': absolute_url
            })
    
    # ‡§°‡•Å‡§™‡•ç‡§≤‡•Ä‡§ï‡•á‡§ü ‡§π‡§ü‡§æ‡§è‡§Ç
    return list({doc['url']: doc for doc in documents}.values())

async def create_document_file(url, documents):
    """‡§°‡•â‡§ï‡•ç‡§Ø‡•Ç‡§Æ‡•á‡§Ç‡§ü‡•ç‡§∏ ‡§ï‡•Ä TXT ‡§´‡§º‡§æ‡§á‡§≤ ‡§¨‡§®‡§æ‡§è‡§Ç"""
    domain = get_domain(url)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{domain}_documents_{timestamp}.txt"
    
    with open(filename, 'w', encoding='utf-8') as f:
        for doc in documents:
            f.write(f"{doc['name']}\n{doc['url']}\n\n")
    
    return filename

async def check_website_updates(client):
    """‡§µ‡•á‡§¨‡§∏‡§æ‡§á‡§ü ‡§Ö‡§™‡§°‡•á‡§ü ‡§ö‡•á‡§ï ‡§ï‡§∞‡•á‡§Ç"""
    user_data = load_user_data()
    for user_id, data in user_data.items():
        for url_info in data['tracked_urls']:
            url = url_info['url']
            stored_hash = url_info['hash']
            stored_documents = url_info['documents']

            current_content = fetch_url_content(url)
            if not current_content:
                continue

            current_hash = hashlib.sha256(current_content.encode()).hexdigest()
            current_documents = extract_documents(current_content, url)

            if current_hash != stored_hash:
                try:
                    # ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§¨‡§¶‡§≤‡§æ‡§µ ‡§∏‡•Ç‡§ö‡§®‡§æ
                    await client.send_message(
                        chat_id=user_id,
                        text=f"üö® ‡§µ‡•á‡§¨‡§∏‡§æ‡§á‡§ü ‡§Æ‡•á‡§Ç ‡§¨‡§¶‡§≤‡§æ‡§µ ‡§Ü‡§Ø‡§æ ‡§π‡•à! {url}"
                    )
                except Exception as e:
                    logger.error(f"Error sending update to {user_id}: {e}")

                # ‡§®‡§è ‡§°‡•â‡§ï‡•ç‡§Ø‡•Ç‡§Æ‡•á‡§Ç‡§ü ‡§ö‡•á‡§ï ‡§ï‡§∞‡•á‡§Ç
                new_docs = [doc for doc in current_documents 
                           if doc not in stored_documents]
                
                if new_docs:
                    try:
                        # TXT ‡§´‡§º‡§æ‡§á‡§≤ ‡§¨‡§®‡§æ‡§ï‡§∞ ‡§≠‡•á‡§ú‡•á‡§Ç
                        txt_file = await create_document_file(url, new_docs)
                        await client.send_document(
                            chat_id=user_id,
                            document=txt_file,
                            caption=f"üìÑ {url} ‡§™‡§∞ ‡§®‡§è ‡§°‡•â‡§ï‡•ç‡§Ø‡•Ç‡§Æ‡•á‡§Ç‡§ü ‡§Æ‡§ø‡§≤‡•á ({len(new_docs)})"
                        )
                        os.remove(txt_file)
                    except Exception as e:
                        logger.error(f"Error sending document to {user_id}: {e}")

                    # ‡§°‡•á‡§ü‡§æ ‡§Ö‡§™‡§°‡•á‡§ü ‡§ï‡§∞‡•á‡§Ç
                    url_info['documents'] = current_documents
                    url_info['hash'] = current_hash
    
    save_user_data(user_data)

async def start(client, message):
    """‡§∏‡•ç‡§ü‡§æ‡§∞‡•ç‡§ü ‡§ï‡§Æ‡§æ‡§Ç‡§° ‡§π‡•à‡§Ç‡§°‡§≤‡§∞"""
    await message.reply_text(
        '‡§µ‡•á‡§¨‡§∏‡§æ‡§á‡§ü ‡§ü‡•ç‡§∞‡•à‡§ï‡§ø‡§Ç‡§ó ‡§¨‡•â‡§ü ‡§Æ‡•á‡§Ç ‡§Ü‡§™‡§ï‡§æ ‡§∏‡•ç‡§µ‡§æ‡§ó‡§§ ‡§π‡•à!\n\n'
        '‡§ï‡§Æ‡§æ‡§Ç‡§°‡•ç‡§∏:\n'
        '/track <url> - ‡§µ‡•á‡§¨‡§∏‡§æ‡§á‡§ü ‡§ü‡•ç‡§∞‡•à‡§ï ‡§ï‡§∞‡•á‡§Ç\n'
        '/untrack <url> - ‡§ü‡•ç‡§∞‡•à‡§ï‡§ø‡§Ç‡§ó ‡§∞‡•ã‡§ï‡•á‡§Ç\n'
        '/list - ‡§ü‡•ç‡§∞‡•à‡§ï ‡§ï‡•Ä ‡§ó‡§à ‡§µ‡•á‡§¨‡§∏‡§æ‡§á‡§ü‡•ç‡§∏ ‡§¶‡•á‡§ñ‡•á‡§Ç\n'
        '/documents <url> - ‡§°‡•â‡§ï‡•ç‡§Ø‡•Ç‡§Æ‡•á‡§Ç‡§ü‡•ç‡§∏ ‡§ï‡•Ä ‡§∏‡•Ç‡§ö‡•Ä ‡§™‡•ç‡§∞‡§æ‡§™‡•ç‡§§ ‡§ï‡§∞‡•á‡§Ç'
    )

async def track(client, message):
    """URL ‡§ü‡•ç‡§∞‡•à‡§ï ‡§ï‡§∞‡•á‡§Ç"""
    user_id = str(message.from_user.id)
    url = ' '.join(message.command[1:]).strip()

    if not url.startswith(('http://', 'https://')):
        await message.reply_text("‚ö† ‡§ï‡•É‡§™‡§Ø‡§æ ‡§µ‡•à‡§ß URL ‡§°‡§æ‡§≤‡•á‡§Ç (http/https ‡§ï‡•á ‡§∏‡§æ‡§•)")
        return

    user_data = load_user_data()
    if user_id not in user_data:
        user_data[user_id] = {'tracked_urls': []}

    if any(u['url'] == url for u in user_data[user_id]['tracked_urls']):
        await message.reply_text("‚ùå ‡§Ø‡§π URL ‡§™‡§π‡§≤‡•á ‡§∏‡•á ‡§ü‡•ç‡§∞‡•à‡§ï ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•à")
        return

    content = fetch_url_content(url)
    if not content:
        await message.reply_text("‚ùå URL ‡§è‡§ï‡•ç‡§∏‡•á‡§∏ ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ ‡§∏‡§ï‡§æ")
        return

    current_hash = hashlib.sha256(content.encode()).hexdigest()
    current_documents = extract_documents(content, url)

    user_data[user_id]['tracked_urls'].append({
        'url': url,
        'hash': current_hash,
        'documents': current_documents
    })

    save_user_data(user_data)
    await message.reply_text(f"‚úÖ ‡§ü‡•ç‡§∞‡•à‡§ï‡§ø‡§Ç‡§ó ‡§∂‡•Å‡§∞‡•Ç: {url}\n‡§Æ‡§ø‡§≤‡•á ‡§°‡•â‡§ï‡•ç‡§Ø‡•Ç‡§Æ‡•á‡§Ç‡§ü‡•ç‡§∏: {len(current_documents)}")

async def untrack(client, message):
    """‡§ü‡•ç‡§∞‡•à‡§ï‡§ø‡§Ç‡§ó ‡§∞‡•ã‡§ï‡•á‡§Ç"""
    user_id = str(message.from_user.id)
    url = ' '.join(message.command[1:]).strip()

    user_data = load_user_data()
    if user_id not in user_data:
        await message.reply_text("‚ùå ‡§ï‡•ã‡§à ‡§ü‡•ç‡§∞‡•à‡§ï ‡§ï‡§ø‡§è ‡§ó‡§è URL ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡•á")
        return

    original_count = len(user_data[user_id]['tracked_urls'])
    user_data[user_id]['tracked_urls'] = [
        u for u in user_data[user_id]['tracked_urls']
        if u['url'] != url
    ]

    if len(user_data[user_id]['tracked_urls']) < original_count:
        save_user_data(user_data)
        await message.reply_text(f"‚ùé ‡§ü‡•ç‡§∞‡•à‡§ï‡§ø‡§Ç‡§ó ‡§¨‡§Ç‡§¶: {url}")
    else:
        await message.reply_text("‚ùå URL ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡§æ")

async def list_urls(client, message):
    """‡§ü‡•ç‡§∞‡•à‡§ï ‡§ï‡§ø‡§è ‡§ó‡§è URLs ‡§¶‡§ø‡§ñ‡§æ‡§è‡§Ç"""
    user_id = str(message.from_user.id)
    user_data = load_user_data()

    if user_id not in user_data or not user_data[user_id]['tracked_urls']:
        await message.reply_text("üì≠ ‡§Ü‡§™‡§®‡•á ‡§Ö‡§≠‡•Ä ‡§ï‡•ã‡§à URL ‡§ü‡•ç‡§∞‡•à‡§ï ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§ø‡§Ø‡§æ ‡§π‡•à")
        return

    urls = "\n".join([u['url'] for u in user_data[user_id]['tracked_urls']])
    await message.reply_text(f"üìú ‡§ü‡•ç‡§∞‡•à‡§ï ‡§ï‡§ø‡§è ‡§ó‡§è URLs:\n\n{urls}")

async def list_documents(client, message):
    """‡§°‡•â‡§ï‡•ç‡§Ø‡•Ç‡§Æ‡•á‡§Ç‡§ü‡•ç‡§∏ ‡§ï‡•Ä ‡§∏‡•Ç‡§ö‡•Ä ‡§≠‡•á‡§ú‡•á‡§Ç"""
    user_id = str(message.from_user.id)
    url = ' '.join(message.command[1:]).strip()

    user_data = load_user_data()
    if user_id not in user_data or not user_data[user_id]['tracked_urls']:
        await message.reply_text("‚ùå ‡§Ü‡§™‡§®‡•á ‡§ï‡•ã‡§à URL ‡§ü‡•ç‡§∞‡•à‡§ï ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§ø‡§Ø‡§æ ‡§π‡•à")
        return

    url_info = next((u for u in user_data[user_id]['tracked_urls'] if u['url'] == url), None)
    if not url_info:
        await message.reply_text("‚ùåÔ∏è ‡§Ø‡§π URL ‡§ü‡•ç‡§∞‡•à‡§ï ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à")
        return

    documents = url_info.get('documents', [])
    if not documents:
        await message.reply_text(f"‚ÑπÔ∏è {url} ‡§™‡§∞ ‡§ï‡•ã‡§à ‡§°‡•â‡§ï‡•ç‡§Ø‡•Ç‡§Æ‡•á‡§Ç‡§ü ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡§æ")
    else:
        try:
            txt_file = await create_document_file(url, documents)
            await client.send_document(
                chat_id=user_id,
                document=txt_file,
                caption=f"üìë {url} ‡§ï‡•á ‡§∏‡§≠‡•Ä ‡§°‡•â‡§ï‡•ç‡§Ø‡•Ç‡§Æ‡•á‡§Ç‡§ü‡•ç‡§∏ ({len(documents)})"
            )
            os.remove(txt_file)
        except Exception as e:
            logger.error(f"Error sending documents list: {e}")
            await message.reply_text("‚ùå ‡§°‡•â‡§ï‡•ç‡§Ø‡•Ç‡§Æ‡•á‡§Ç‡§ü‡•ç‡§∏ ‡§≠‡•á‡§ú‡§®‡•á ‡§Æ‡•á‡§Ç ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø")

def main():
    """‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§è‡§™‡•ç‡§≤‡§ø‡§ï‡•á‡§∂‡§®"""
    app = Client(
        "my_bot",
        api_id="YOUR_API_ID",
        api_hash="YOUR_API_HASH",
        bot_token="YOUR_BOT_TOKEN"
    )

    # ‡§ï‡§Æ‡§æ‡§Ç‡§° ‡§π‡•à‡§Ç‡§°‡§≤‡§∞‡•ç‡§∏
    handlers = [
        MessageHandler(start, filters.command("start")),
        MessageHandler(track, filters.command("track")),
        MessageHandler(untrack, filters.command("untrack")),
        MessageHandler(list_urls, filters.command("list")),
        MessageHandler(list_documents, filters.command("documents"))
    ]
    
    for handler in handlers:
        app.add_handler(handler)

    # ‡§∂‡•á‡§°‡•ç‡§Ø‡•Ç‡§≤‡§∞ ‡§∏‡•á‡§ü‡§Ö‡§™
    scheduler = BackgroundScheduler()
    scheduler.add_job(check_website_updates, 'interval', minutes=30, args=[app])
    scheduler.start()

    try:
        app.run()
    except Exception as e:
        logger.error(f"‡§¨‡•â‡§ü ‡§ö‡§≤‡§æ‡§®‡•á ‡§Æ‡•á‡§Ç ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø: {e}")

if __name__ == '__main__':
    main()
